{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "image 1/1 c:\\Users\\s.j\\Desktop\\Ali Asghar\\Automatic-Grading\\POC\\Line Extractor\\images\\handwritten7.png: 480x800 4 lines, 402.0ms\n",
      "Speed: 4.0ms preprocess, 402.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "C:\\Users\\s.j\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from Bounding Box 1: \" our team for this project. \" it owns kills will \"\n",
      "Text from Bounding Box 2: rainly be survived assistant firm looking -\n",
      "Text from Bounding Box 3: Welcome onboard. I'm very excited to have you\n",
      "Text from Bounding Box 4: ward to seeing what you come up with\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def extract_text_from_boxes(boxes, image, trocr_processor, trocr_model):\n",
    "    extracted_text = []\n",
    "\n",
    "    for box in boxes:\n",
    "        # Extract the region within the bounding box\n",
    "        x1, y1, x2, y2 = (\n",
    "            int(box[\"box\"][\"x1\"]),\n",
    "            int(box[\"box\"][\"y1\"]),\n",
    "            int(box[\"box\"][\"x2\"]),\n",
    "            int(box[\"box\"][\"y2\"]),\n",
    "        )\n",
    "\n",
    "        # Crop the bounding box area from the original image\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert the cropped image to PIL format\n",
    "        cropped_pil_image = Image.fromarray(cropped_image)\n",
    "\n",
    "        # Preprocess the image for TrOCR\n",
    "        pixel_values = trocr_processor(cropped_pil_image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Generate text using TrOCR\n",
    "        generated_ids = trocr_model.generate(pixel_values)\n",
    "        generated_text = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        extracted_text.append(generated_text)\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def main():\n",
    "    # Load the YOLO model\n",
    "    yolo_model = YOLO(\"best.pt\")\n",
    "\n",
    "    # Load the TrOCR model\n",
    "    trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "    trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "    input_path = 'images/handwritten7.png'\n",
    "\n",
    "    # Run inference using the YOLO model\n",
    "    results = yolo_model(input_path)\n",
    "    image = cv2.imread(input_path)\n",
    "\n",
    "    # Convert the results to JSON format\n",
    "    results = results[0].tojson()\n",
    "    result = json.loads(results)\n",
    "\n",
    "    # Extract text from bounding boxes and print\n",
    "    extracted_text = extract_text_from_boxes(result, image, trocr_processor, trocr_model)\n",
    "\n",
    "    # Print the extracted text\n",
    "    for i, text in enumerate(extracted_text):\n",
    "        print(f\"Text from Bounding Box {i + 1}: {text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "image 1/1 c:\\Users\\s.j\\Desktop\\Ali Asghar\\Automatic-Grading\\POC\\Line Extractor\\images\\not_working.jpg: 768x800 12 0s, 538.0ms\n",
      "Speed: 9.0ms preprocess, 538.0ms inference, 2.0ms postprocess per image at shape (1, 3, 768, 800)\n",
      "C:\\Users\\s.j\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from Bounding Box 1: degree the Germans wife members of eligibility\n",
      "Text from Bounding Box 2: future, things with state, care of hope. Children\n",
      "Text from Bounding Box 3: \" He'll smell, in this children, the fighting\n",
      "Text from Bounding Box 4: fabriate which has husband, elected : \"\n",
      "Text from Bounding Box 5: \" She does not, want to abandon them.\n",
      "Text from Bounding Box 6: fate she wind like worn - later cotton\n",
      "Text from Bounding Box 7: \" foods. She, with her all, courage, embraces\n",
      "Text from Bounding Box 8: jimbabwe, death, but, to bravely face,\n",
      "Text from Bounding Box 9: \" be alive \" etc teach her children not\n",
      "Text from Bounding Box 10: its fight with a clenched fist for not\n",
      "Text from Bounding Box 11: only like basic need of food but also,\n",
      "Text from Bounding Box 12: to tell the movement from about\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Define a function to sort and filter bounding boxes\n",
    "def sort_and_filter_boxes(boxes):\n",
    "    # Sort boxes based on their y-axis attribute\n",
    "    sorted_boxes = sorted(boxes, key=lambda x: x[\"box\"][\"y1\"])\n",
    "\n",
    "    # Filter boxes based on confidence (e.g., keep boxes with confidence > 0.5)\n",
    "    # filtered_boxes = [box for box in sorted_boxes if box[\"confidence\"] > 0.5]\n",
    "\n",
    "    return sorted_boxes\n",
    "\n",
    "def extract_text_from_boxes(boxes, image, trocr_processor, trocr_model):\n",
    "    extracted_text = []\n",
    "\n",
    "    for box in boxes:\n",
    "        # Extract the region within the bounding box\n",
    "        x1, y1, x2, y2 = (\n",
    "            int(box[\"box\"][\"x1\"]),\n",
    "            int(box[\"box\"][\"y1\"]),\n",
    "            int(box[\"box\"][\"x2\"]),\n",
    "            int(box[\"box\"][\"y2\"]),\n",
    "        )\n",
    "\n",
    "        # Crop the bounding box area from the original image\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert the cropped image to PIL format\n",
    "        cropped_pil_image = Image.fromarray(cropped_image)\n",
    "\n",
    "        # Preprocess the image for TrOCR\n",
    "        pixel_values = trocr_processor(cropped_pil_image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Generate text using TrOCR\n",
    "        generated_ids = trocr_model.generate(pixel_values)\n",
    "        generated_text = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        extracted_text.append(generated_text)\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def main():\n",
    "    # Load the YOLO model\n",
    "    yolo_model = YOLO(\"model/bangla+iam.pt\")\n",
    "\n",
    "    # Load the TrOCR model\n",
    "    trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "    trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "    input_path = 'images/not_working.jpg'\n",
    "\n",
    "    # Run inference using the YOLO model\n",
    "    results = yolo_model(input_path, conf=0.45)\n",
    "    image = cv2.imread(input_path)\n",
    "\n",
    "    # Convert the results to JSON format\n",
    "    results = results[0].tojson()\n",
    "    result = json.loads(results)\n",
    "\n",
    "    # Sort and filter bounding boxes\n",
    "    sorted_and_filtered_boxes = sort_and_filter_boxes(result)\n",
    "\n",
    "    # Extract text from sorted and filtered bounding boxes\n",
    "    extracted_text = extract_text_from_boxes(sorted_and_filtered_boxes, image, trocr_processor, trocr_model)\n",
    "\n",
    "    # Print the extracted text\n",
    "    for i, text in enumerate(extracted_text):\n",
    "        print(f\"Text from Bounding Box {i + 1}: {text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "image 1/1 c:\\Users\\s.j\\Desktop\\Ali Asghar\\Automatic-Grading\\POC\\Line Extractor\\images\\not_working.jpg: 768x800 21 0s, 547.0ms\n",
      "Speed: 7.0ms preprocess, 547.0ms inference, 1.0ms postprocess per image at shape (1, 3, 768, 800)\n",
      "C:\\Users\\s.j\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from Bounding Box 1: degree the Germans wife members of eligibility\n",
      "Text from Bounding Box 2: future, things with state, care of hope. Children\n",
      "Text from Bounding Box 3: gate, believe that state care of huge child\n",
      "Text from Bounding Box 4: \" What thou know it of the twentful thing caught\n",
      "Text from Bounding Box 5: \" He'll smell, in this children, the fighting\n",
      "Text from Bounding Box 6: fabriate which has husband, elected : \"\n",
      "Text from Bounding Box 7: \" She does not, want to abandon them.\n",
      "Text from Bounding Box 8: fate she wind like worn - later cotton\n",
      "Text from Bounding Box 9: \" foods. She, with her all, courage, embraces\n",
      "Text from Bounding Box 10: flipped and the twelfth tier children's most notorious\n",
      "Text from Bounding Box 11: \" Life and \" teach her children. \" not to\n",
      "Text from Bounding Box 12: jimbabwe, death, but, to bravely face,\n",
      "Text from Bounding Box 13: flifle and its struggles that \" is why\n",
      "Text from Bounding Box 14: she says that she would continue to\n",
      "Text from Bounding Box 15: \" be alive \" etc teach her children not\n",
      "Text from Bounding Box 16: its fight with a clenched fist for not\n",
      "Text from Bounding Box 17: only like basic need of food but also,\n",
      "Text from Bounding Box 18: meil important, things, \" attaining which\n",
      "Text from Bounding Box 19: right, but nothing like them a ball\n",
      "Text from Bounding Box 20: will be named as shown I choose\n",
      "Text from Bounding Box 21: to tell the movement from about\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "class ImgCorrect():\n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "        self.h, self.w, self.channel = self.img.shape\n",
    "        if self.w <= self.h:\n",
    "            self.scale = 700 / self.w\n",
    "            self.img = cv2.resize(self.img, (0, 0), fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            self.scale = 700 / self.h\n",
    "            self.img = cv2.resize(self.img, (0, 0), fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "        self.gray = cv2.cvtColor(self.img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    def img_lines(self):\n",
    "        ret, binary = cv2.threshold(self.gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        binary = cv2.dilate(binary, kernel)\n",
    "        edges = cv2.Canny(binary, 50, 200)\n",
    "\n",
    "        self.lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 100, minLineLength=100, maxLineGap=20)\n",
    "\n",
    "        if self.lines is None:\n",
    "            return None\n",
    "\n",
    "        lines1 = self.lines[:, 0, :]\n",
    "        imglines = self.img.copy()\n",
    "        for x1, y1, x2, y2 in lines1[:]:\n",
    "            cv2.line(imglines, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        return imglines\n",
    "\n",
    "    def search_lines(self):\n",
    "        lines = self.lines[:, 0, :]\n",
    "        number_inexist_k = 0\n",
    "        sum_pos_k45 = number_pos_k45 = 0\n",
    "        sum_pos_k90 = number_pos_k90 = 0\n",
    "        sum_neg_k45 = number_neg_k45 = 0\n",
    "        sum_neg_k90 = number_neg_k90 = 0\n",
    "        sum_zero_k = number_zero_k = 0\n",
    "\n",
    "        for x in lines:\n",
    "            if x[2] == x[0]:\n",
    "                number_inexist_k += 1\n",
    "                continue\n",
    "            degree = np.degrees(np.arctan((x[3] - x[1]) / (x[2] - x[0])))\n",
    "            if 0 < degree < 45:\n",
    "                number_pos_k45 += 1\n",
    "                sum_pos_k45 += degree\n",
    "            if 45 <= degree < 90:\n",
    "                number_pos_k90 += 1\n",
    "                sum_pos_k90 += degree\n",
    "            if -45 < degree < 0:\n",
    "                number_neg_k45 += 1\n",
    "                sum_neg_k45 += degree\n",
    "            if -90 < degree <= -45:\n",
    "                number_neg_k90 += 1\n",
    "                sum_neg_k90 += degree\n",
    "            if x[3] == x[1]:\n",
    "                number_zero_k += 1\n",
    "\n",
    "        max_number = max(number_inexist_k, number_pos_k45, number_pos_k90, number_neg_k45, number_neg_k90, number_zero_k)\n",
    "\n",
    "        if max_number == number_inexist_k:\n",
    "            return 90\n",
    "        if max_number == number_pos_k45:\n",
    "            return sum_pos_k45 / number_pos_k45\n",
    "        if max_number == number_pos_k90:\n",
    "            return sum_pos_k90 / number_pos_k90\n",
    "        if max_number == number_neg_k45:\n",
    "            return sum_neg_k45 / number_neg_k45\n",
    "        if max_number == number_neg_k90:\n",
    "            return sum_neg_k90 / number_neg_k90\n",
    "        if max_number == number_zero_k:\n",
    "            return 0\n",
    "\n",
    "    def rotate_image(self, degree):\n",
    "        if -45 <= degree <= 0:\n",
    "            degree = degree\n",
    "        if -90 <= degree < -45:\n",
    "            degree = 90 + degree\n",
    "        if 0 < degree <= 45:\n",
    "            degree = degree\n",
    "        if 45 < degree <= 90:\n",
    "            degree = degree - 90\n",
    "\n",
    "        height, width = self.img.shape[:2]\n",
    "        heightNew = int(width * np.abs(np.sin(np.radians(degree))) + height * np.abs(np.cos(np.radians(degree))))\n",
    "        widthNew = int(height * np.abs(np.sin(np.radians(degree))) + width * np.abs(np.cos(np.radians(degree))))\n",
    "\n",
    "        matRotation = cv2.getRotationMatrix2D((width / 2, height / 2), degree, 1)\n",
    "        matRotation[0, 2] += (widthNew - width) / 2\n",
    "        matRotation[1, 2] += (heightNew - height) / 2\n",
    "\n",
    "        imgRotation = cv2.warpAffine(self.img, matRotation, (widthNew, heightNew), borderValue=(255, 255, 255))\n",
    "\n",
    "        bg_color = [255, 255, 255]\n",
    "        pad_image_rotate = cv2.warpAffine(self.img, matRotation, (widthNew, heightNew), borderValue=(0, 255, 0))\n",
    "\n",
    "        return pad_image_rotate\n",
    "\n",
    "def dskew(line_path, img):\n",
    "    img_loc = line_path + img\n",
    "    im = cv2.imread(img_loc)\n",
    "    bg_color = [255, 255, 255]\n",
    "    pad_img = cv2.copyMakeBorder(im, 100, 100, 100, 100, cv2.BORDER_CONSTANT, value=bg_color)\n",
    "    imgcorrect = ImgCorrect(pad_img)\n",
    "    lines_img = imgcorrect.img_lines()\n",
    "\n",
    "    if lines_img is None:\n",
    "        rotate = imgcorrect.rotate_image(0)\n",
    "    else:\n",
    "        degree = imgcorrect.search_lines()\n",
    "        rotate = imgcorrect.rotate_image(degree)\n",
    "\n",
    "    return rotate\n",
    "\n",
    "# Define a function to sort and filter bounding boxes\n",
    "def sort_and_filter_boxes(boxes):\n",
    "    # Sort boxes based on their y-axis attribute\n",
    "    sorted_boxes = sorted(boxes, key=lambda x: x[\"box\"][\"y1\"])\n",
    "\n",
    "    return sorted_boxes\n",
    "\n",
    "def extract_text_from_boxes(boxes, image, trocr_processor, trocr_model):\n",
    "    extracted_text = []\n",
    "\n",
    "    for box in boxes:\n",
    "        # Extract the region within the bounding box\n",
    "        x1, y1, x2, y2 = (\n",
    "            int(box[\"box\"][\"x1\"]),\n",
    "            int(box[\"box\"][\"y1\"]),\n",
    "            int(box[\"box\"][\"x2\"]),\n",
    "            int(box[\"box\"][\"y2\"]),\n",
    "        )\n",
    "\n",
    "        # Crop the bounding box area from the original image\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert the cropped image to PIL format\n",
    "        cropped_pil_image = Image.fromarray(cropped_image)\n",
    "\n",
    "        # Preprocess the image for TrOCR\n",
    "        pixel_values = trocr_processor(cropped_pil_image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Generate text using TrOCR\n",
    "        generated_ids = trocr_model.generate(pixel_values)\n",
    "        generated_text = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        extracted_text.append(generated_text)\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def main():\n",
    "    # Load the YOLO model\n",
    "    yolo_model = YOLO(\"model/bangla+iam.pt\")\n",
    "\n",
    "    # Load the TrOCR model\n",
    "    trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "    trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "    input_path = 'images/not_working.jpg'\n",
    "\n",
    "    # Run inference using the YOLO model\n",
    "    results = yolo_model(input_path)\n",
    "    image = cv2.imread(input_path)\n",
    "\n",
    "    # Convert the results to JSON format\n",
    "    results = results[0].tojson()\n",
    "    result = json.loads(results)\n",
    "\n",
    "    # Sort and filter bounding boxes\n",
    "    sorted_and_filtered_boxes = sort_and_filter_boxes(result)\n",
    "\n",
    "    # Extract text from sorted and filtered bounding boxes\n",
    "    extracted_text = extract_text_from_boxes(sorted_and_filtered_boxes, image, trocr_processor, trocr_model)\n",
    "\n",
    "    # Print the extracted text\n",
    "    for i, text in enumerate(extracted_text):\n",
    "        print(f\"Text from Bounding Box {i + 1}: {text}\")\n",
    "\n",
    "    # Skew correction using ImgCorrect class\n",
    "    line_path = input_path.split('/')[0] + \"/\"  # Change this to your desired directory\n",
    "    img_name = input_path.split('/')[1]  # Change this to the image you want to correct\n",
    "\n",
    "    # Perform skew correction\n",
    "    corrected_image = dskew(line_path, img_name)\n",
    "\n",
    "    # Display the corrected image\n",
    "    cv2.imshow('Corrected Image', corrected_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
